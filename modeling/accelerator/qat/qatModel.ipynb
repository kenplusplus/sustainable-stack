{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the csv file \n",
    "Read in the dataset and create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace \"CSVFILE\" with your own CSV file path\n",
    "data = pd.read_csv(\"CSVFILE\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use .describe() to see some summary statistics for the numeric fields in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split the dataset for ML\n",
    "Split the dataset into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.25)\n",
    "train, val = train_test_split(train, test_size=0.33)\n",
    "\n",
    "print(len(train), \"train datas\")\n",
    "print(len(val), \"validation datas\")\n",
    "print(len(test), \"test datas\")\n",
    "\n",
    "train.to_csv(\"./train.csv\")\n",
    "val.to_csv(\"./val.csv\")\n",
    "test.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset using StandardScaler.\n",
    "def normalize_fn(dataset):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_dataset = [\n",
    "        pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "        for data in dataset\n",
    "    ]\n",
    "    return normalized_dataset\n",
    "\n",
    "\n",
    "# Convert a Pandas Dataframe to tf.tensor\n",
    "def tensor_fn(dataset, dtype=tf.float64):\n",
    "    tensor_dataset = [tf.convert_to_tensor(data.values, dtype) for data in dataset]\n",
    "\n",
    "    return tensor_dataset\n",
    "\n",
    "\n",
    "# Create a normalized tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=48):\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Drop the column 'file_name'\n",
    "    dataframe = dataframe.drop(\"file_name\", axis=1)\n",
    "\n",
    "    # Split dataframe into features and labels\n",
    "    labels = dataframe[[\"pkg_energy\", \"dram_energy\"]]\n",
    "    features = dataframe.drop(columns=[\"pkg_energy\", \"dram_energy\"])\n",
    "\n",
    "    normalized_ds = normalize_fn([features, labels])\n",
    "    tensor_ds = tensor_fn(normalized_ds)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((tensor_ds[0], tensor_ds[1]))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Initialize the training, validation and testing datasets.\n",
    "batch_size = 48\n",
    "train_ds = df_to_dataset(train)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Keras Sequential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function R-squared as a metric for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_total = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - ss_res / (ss_total + tf.keras.backend.epsilon())\n",
    "    return r2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and Fit the Keras Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model create\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input((10,)),\n",
    "        tf.keras.layers.Dense(24, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(12, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model complie\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss = tf.keras.losses.Huber(delta=1.35)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=r_squared)\n",
    "\n",
    "# Model fit\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, metrics):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    for idx, key in enumerate(metrics):\n",
    "        ax = fig.add_subplot(nrows, ncols, idx + 1)\n",
    "        plt.plot(history.history[key])\n",
    "        plt.plot(history.history[\"val_{}\".format(key)])\n",
    "        plt.title(\"model {}\".format(key))\n",
    "        plt.ylabel(key)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.legend([\"train\", \"validation\"])\n",
    "\n",
    "plot_history(history, [\"loss\", \"r_squared\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and summary the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluate\n",
    "loss, r_squred = model.evaluate(test_ds)\n",
    "\n",
    "print(\"loss:\", loss)\n",
    "print(\"r_squred:\", r_squred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
